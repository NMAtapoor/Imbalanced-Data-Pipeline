{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Abalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EPIC 1 STORY 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4174 entries, 0 to 4173\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Sex              4174 non-null   object \n",
      " 1    Length          4174 non-null   float64\n",
      " 2    Diameter        4174 non-null   float64\n",
      " 3    Height          4174 non-null   float64\n",
      " 4    Whole_weight    4174 non-null   float64\n",
      " 5    Shucked_weight  4174 non-null   float64\n",
      " 6    Viscera_weight  4174 non-null   float64\n",
      " 7    Shell_weight    4174 non-null   float64\n",
      " 8   Class            4174 non-null   object \n",
      "dtypes: float64(7), object(2)\n",
      "memory usage: 293.6+ KB\n"
     ]
    }
   ],
   "source": [
    "abalone_df = pd.read_csv('../data/raw/unclean_abalone.csv')\n",
    "\n",
    "abalone_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4174 entries, 0 to 4173\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Sex              4174 non-null   string \n",
      " 1    Length          4174 non-null   float64\n",
      " 2    Diameter        4174 non-null   float64\n",
      " 3    Height          4174 non-null   float64\n",
      " 4    Whole_weight    4174 non-null   float64\n",
      " 5    Shucked_weight  4174 non-null   float64\n",
      " 6    Viscera_weight  4174 non-null   float64\n",
      " 7    Shell_weight    4174 non-null   float64\n",
      " 8   Class            4174 non-null   string \n",
      "dtypes: float64(7), string(2)\n",
      "memory usage: 293.6 KB\n"
     ]
    }
   ],
   "source": [
    "abalone_df[[\"Class\",\"Sex\"]] = abalone_df[[\"Class\",\"Sex\"]].astype(\"string\")\n",
    "abalone_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV imported is the one that the customer has provided.  We will explore it and then create a cleaned data file that can be used in testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is some missing data in the `age`, `country` and `is_active` columns.  We will need to clean this data before we can use it for analysis or further processing.  We can see how many rows we expect to lose as a result cleaning this.\n",
    "\n",
    "> NOTE: in this instance, the customer isn't bothered that the `age` column has missing values, so we will be removing this column from the final cleaned data set.\n",
    "\n",
    "Let's find out how many rows we will actually lose..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many rows have both is_active and country missing\n",
    "# Count rows with just country missing\n",
    "just_country_missing = (\n",
    "    customers[\"country\"].isnull() & customers[\"is_active\"].notnull()\n",
    ")\n",
    "print(f\"Just country missing: {just_country_missing.sum()}\")\n",
    "\n",
    "# Count rows with just is_active missing\n",
    "just_is_active_missing = (\n",
    "    customers[\"is_active\"].isnull() & customers[\"country\"].notnull()\n",
    ")\n",
    "print(f\"Just is_active missing: {just_is_active_missing.sum()}\")\n",
    "\n",
    "# Count rows with both country and is_active missing\n",
    "both_missing = customers[\"country\"].isnull() & customers[\"is_active\"].isnull()\n",
    "print(f\"Both missing: {both_missing.sum()}\")\n",
    "\n",
    "print(\n",
    "    f\"Total number of rows lost will be {\n",
    "    customers[['country', 'is_active']].isnull().any(axis=1).sum()\n",
    "}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***477*** rows is around ***9%*** of the data, so we will be losing a significant amount of data.\n",
    "> Our customer is happy with this as the majority are those with `is_active` missing, so they would be considered ***inactive*** anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `is_active` Column\n",
    "\n",
    "Lets take a look at the `is_active` column's values.  We'll get the first 20 unique values to see if there are any inconsistencies or unexpected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 20 unique values from the is_active column\n",
    "\n",
    "customers[\"is_active\"].unique()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if all values are string that are not nan\n",
    "\n",
    "for value in customers[\"is_active\"].unique()[:20]:\n",
    "    print(f\"Value: {repr(value)}, Type: {type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this data is not standardised, although it is all `'str'` (we ignore the `nan` values as they will be removed with missing values).  We will use the following mapping for standardisation:\n",
    "\n",
    "| Value    | Standardised Value |\n",
    "|----------|--------------------|\n",
    "| 1        | True               |\n",
    "| 0        | False              |\n",
    "| active   | True               |\n",
    "| inactive | False              |\n",
    "| False    | False              |\n",
    "| True     | True               |\n",
    "\n",
    "`nan` values will be removed prior to standardisation, as they are not valid values for this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicates\n",
    "\n",
    "How many duplicates are there in the data?  Let's get a ball-park figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate rows in current data set\n",
    "customers.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as though we have ***149*** duplcate rows in the data set.  This is around ***3%*** of the data, so we will be losing a small amount of data.\n",
    "\n",
    "> We will need to check how many duplicates again once we have removed the missing values, as some of these may be duplicates of rows that have missing values in the `is_active` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epic 3 - Story 4 - Task 2 - Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in country and is_active\n",
    "\n",
    "customers = customers.dropna(subset=['country', 'is_active'])\n",
    "\n",
    "customers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's ***477*** rows removed as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a check - this operation will be tested in the pipeline!\n",
    "just_country_missing = (\n",
    "    customers[\"country\"].isnull() & customers[\"is_active\"].notnull()\n",
    ")\n",
    "print(f\"Just country missing: {just_country_missing.sum()}\")\n",
    "\n",
    "# Count rows with just is_active missing\n",
    "just_is_active_missing = (\n",
    "    customers[\"is_active\"].isnull() & customers[\"country\"].notnull()\n",
    ")\n",
    "print(f\"Just is_active missing: {just_is_active_missing.sum()}\")\n",
    "\n",
    "# Count rows with both country and is_active missing\n",
    "both_missing = customers[\"country\"].isnull() & customers[\"is_active\"].isnull()\n",
    "print(f\"Both missing: {both_missing.sum()}\")\n",
    "\n",
    "print(\n",
    "    f\"Total number of rows lost will be {\n",
    "    customers[['country', 'is_active']].isnull().any(axis=1).sum()\n",
    "}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epic 3 - Story 4 - Task 3 - Remove `age` Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the `age` column from the dataset\n",
    "customers = customers.drop(columns=['age'])\n",
    "\n",
    "customers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 3 - Story 4 - Task 4 - Standardise `is_active` Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise is_active\n",
    "mapping = {\n",
    "    \"1\": True,\n",
    "    \"0\": False,\n",
    "    \"active\": True,\n",
    "    \"inactive\": False,\n",
    "    \"False\": False,\n",
    "    \"True\": True,\n",
    "}\n",
    "\n",
    "# Fill nan with False first, then convert to bool\n",
    "customers[\"is_active\"] = (\n",
    "    customers[\"is_active\"]\n",
    "    .map(mapping)\n",
    "    .fillna(False)\n",
    "    .infer_objects()\n",
    "    .astype(bool)\n",
    ")\n",
    "\n",
    "\n",
    "print(customers.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in is_active\n",
    "\n",
    "print(customers['is_active'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if all values are string that are not nan\n",
    "\n",
    "for value in customers[\"is_active\"].unique()[:20]:\n",
    "    print(f\"Value: {repr(value)}, Type: {type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just boolean `True` or `False` values are now present in the `is_active` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "\n",
    "customers = customers.drop_duplicates()\n",
    "\n",
    "print(customers.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's ***136*** duplicated removed - we must have lost 13 of them along the way! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the indexes\n",
    "\n",
    "> This was added after the previous task as COMPONENT tests failed due to index conflicts - the same will be added for the transaction data set.\n",
    "\n",
    "We can see that the indexes are now out of order, so we will reset them to be sequential again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 4 - Task 6 - Save the Cleaned Data\n",
    "\n",
    "For testing purposes in the pipeline, it makes sense for us to export the cleaned DataFrame to a CSV file.  This will allow us to use the cleaned data in the pipeline without having to run the cleaning steps again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.to_csv('../tests/test_data/expected_customers_clean_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 4 - Task 7 - Transfer the code from the Jupyter Notebook to a Python script, creating separate functions for each cleaning step\n",
    "\n",
    "### Epic 2 - Story 4 - Task 8 - Write tests for each cleaning function to ensure they work correctly\n",
    "\n",
    "### Epic 2 - Story 4 - Task 9 - Create a script to run the cleaning functions in sequence and log the process\n",
    "\n",
    "### Epic 2 - Story 4 - Task 10 - Add the customer cleaning script to scripts/run and update any tests accordingly\n",
    "\n",
    "Jupyter Notebooks do not play nicely with CI/CD pipelines, so we will need to transfer the code from the Jupyter Notebook to a Python script.  We will create separate functions for each cleaning step and then write tests for each function to ensure they work correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
