{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file has been added to `data/raw` with the name `unclean_transactions.csv`.  This file contains an export of the transactions table from the database.  We'll use this for our exploratory data analysis and transformation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10500 entries, 0 to 10499\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   transaction_id    10500 non-null  int64 \n",
      " 1   customer_id       10500 non-null  int64 \n",
      " 2   transaction_date  10292 non-null  object\n",
      " 3   amount            10454 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 328.3+ KB\n"
     ]
    }
   ],
   "source": [
    "transactions = pd.read_csv('../data/raw/unclean_transactions.csv')\n",
    "\n",
    "# Display some information about the transactions DataFrame\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is some missing data in the `transaction_date` and `amount` columns.  We will need to clean this data before we can use it for analysis or further processing.  We can see how many rows we expect to lose as a result cleaning this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(254)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "254 rows have missing values in any of the columns.  This is about 0.5% of the total number of rows in the table, so we can safely remove these rows without losing too much data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 518 duplicate rows in the transactions DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Show how many duplicates there are in the DataFrame\n",
    "duplicates = transactions.duplicated().sum()\n",
    "print(f'There are {duplicates} duplicate rows in the transactions DataFrame.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that there are some duplicate rows in the transactions DataFrame.  We will need to remove these duplicates before we can use the data for analysis or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different date formats in transaction_date: ['Mar 05, 2024' '03-01-2024' '16 Oct 2024' 'Mar 06, 2022' '2024-10-19'\n",
      " 'Apr 17, 2023' '2023-01-31' '23 Jul 2024' '10/04/2023' '15-08-2023'\n",
      " '22-07-2023' '2024-04-19' '2022-02-24' 'Nov 12, 2023' '2022-01-20'\n",
      " '24 Jan 2022' '04 Oct 2023' 'Jan 10, 2025' '12/10/2023' 'Aug 23, 2023']\n"
     ]
    }
   ],
   "source": [
    "# Show the different date formats in the transaction_date column\n",
    "unique_date_formats = transactions['transaction_date'].unique()\n",
    "print(f'Different date formats in transaction_date: {unique_date_formats[:20]}')  # Show first 20 unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we will need to standardise the date format in the `transaction_date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different data types in amount column: [<class 'str'> <class 'float'>]\n"
     ]
    }
   ],
   "source": [
    "# Show the different data types in the amount column\n",
    "amount_types = transactions['amount'].map(type).unique()\n",
    "print(f'Different data types in amount column: {amount_types}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the amount is either a string or a float.  We will need to convert the amount column to a numeric type before we can use it for analysis or further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 2 - Handle Missing Values\n",
    "\n",
    "We are going to remove any incomplete rows from the DataFrame.  This will remove any rows that have missing values in any of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10246 entries, 0 to 10499\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   transaction_id    10246 non-null  int64 \n",
      " 1   customer_id       10246 non-null  int64 \n",
      " 2   transaction_date  10246 non-null  object\n",
      " 3   amount            10246 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 400.2+ KB\n"
     ]
    }
   ],
   "source": [
    "transactions = transactions.dropna(subset=[\"transaction_date\"])\n",
    "\n",
    "# remove rows with null values in amount from the transaction dataframe\n",
    "transactions = transactions.dropna(subset=[\"amount\"])\n",
    "\n",
    "# See information about the transactions dataframe\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have dropped 254 rows from the DataFrame that had missing values in any of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run a check - this operation will be tested in the pipeline!\n",
    "transactions.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 3 - Standardise Date Format\n",
    "\n",
    "We saw that the `transaction_date` column has a mix of date formats.  We will standardise this to a single format.  You need to examine the date column and then identify and list ALL of the date formats that are present in the column.  You can then use this information to standardise the date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10246 entries, 0 to 10499\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   transaction_id    10246 non-null  int64 \n",
      " 1   customer_id       10246 non-null  int64 \n",
      " 2   transaction_date  10246 non-null  object\n",
      " 3   amount            10246 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 400.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Convert all dates into dd/mm/yyyy format - write function to handle the different types of date formats\n",
    "def standardise_date(date_str):\n",
    "    if pd.isna(date_str) or date_str == \"\":\n",
    "        return pd.NaT\n",
    "\n",
    "    formats = [\n",
    "        \"%Y/%m/%d\",\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%d %b %Y\",\n",
    "        \"%b %d, %Y\",\n",
    "        \"%d %B %Y\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%m/%d/%Y\",\n",
    "        \"%d/%m/%Y\",\n",
    "    ]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return pd.NaT\n",
    "\n",
    "\n",
    "# Apply the parse_date function to the transaction_date column\n",
    "transactions[\"transaction_date\"] = transactions[\"transaction_date\"].apply(\n",
    "    standardise_date\n",
    ")\n",
    "transactions[\"transaction_date\"] = transactions[\n",
    "    \"transaction_date\"\n",
    "].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "transactions = transactions.dropna(subset=[\"transaction_date\"])\n",
    "\n",
    "# Display the DataFrame info\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dates in the `transaction_date` column are now in the standardised format of `%d/%m/%Y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 4 - Convert Amount to Numeric\n",
    "\n",
    "Find out how many rows can be converted to numeric values in the `amount` column.  This will help us understand how many rows we can use for analysis or further processing.  We specifically want to convert the string to a float and understand how many rows will be converted and how many `NaN` values will be created as a result of this conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convertible: 10205, NaNs: 41\n"
     ]
    }
   ],
   "source": [
    "# Count convertible vs non-convertible amounts\n",
    "convertible = (\n",
    "    pd.to_numeric(transactions[\"amount\"], errors=\"coerce\").notna().sum()\n",
    ")\n",
    "print(f\"Convertible: {convertible}, NaNs: {len(transactions) - convertible}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values that generate the `NaN` values are those that cannot be converted to a float.  This includes any non-numeric characters or strings that do not represent a valid number.  We can check what these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INVALID']\n"
     ]
    }
   ],
   "source": [
    "# Show original values that can't be converted to numeric\n",
    "unconvertible = transactions[\n",
    "    pd.to_numeric(transactions[\"amount\"], errors=\"coerce\").isna()\n",
    "][\"amount\"].unique()\n",
    "print(unconvertible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms to use that a number of rows in the `amount` column cannot be converted to a float as they contain the string `INVALID`.  We can safely remove these rows from the DataFrame as they will not be useful for analysis or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10205 entries, 0 to 10499\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   transaction_id    10205 non-null  int64  \n",
      " 1   customer_id       10205 non-null  int64  \n",
      " 2   transaction_date  10205 non-null  object \n",
      " 3   amount            10205 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 398.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Convert and drop NaNs in one operation\n",
    "transactions[\"amount\"] = pd.to_numeric(transactions[\"amount\"], errors=\"coerce\")\n",
    "transactions.dropna(subset=[\"amount\"], inplace=True)\n",
    "\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have cleaned the `amount` column by converting it to a numeric type and dropping any rows that contain `NaN` values in this column.  This will allow us to use the `amount` column for analysis or further processing.\n",
    "\n",
    "This concluldes the cleaning of the `transactions` DataFrame.  We have removed any rows with missing values, standardised the date format, removed duplicates, and converted the `amount` column to a numeric type.  The DataFrame is now ready for analysis or further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 5 - Remove Duplicates\n",
    "\n",
    "Check to see how many duplicates there are now once the data has been cleaned a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 503 duplicate rows in the transactions DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Show how many duplicates there are in the DataFrame\n",
    "duplicates = transactions.duplicated().sum()\n",
    "print(f\"There are {duplicates} duplicate rows in the transactions DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9702 entries, 0 to 9981\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   transaction_id    9702 non-null   int64  \n",
      " 1   customer_id       9702 non-null   int64  \n",
      " 2   transaction_date  9702 non-null   object \n",
      " 3   amount            9702 non-null   float64\n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 379.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Drop the duplicates\n",
    "\n",
    "transactions.drop_duplicates(inplace=True)\n",
    "\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should have removed the 503 duplicated rows from the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the indexes\n",
    "\n",
    "> This was added after the COMPONENT tests for customers data failed due to index conflicts - as we modified the DataFrame, the indexes were no longer sequential.\n",
    "\n",
    "We can see that the indexes are now out of order, so we will reset them to be sequential again.\n",
    "\n",
    "The COMPONENT tests for the transactions data set will also need to be updated/added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 6 - Save the Cleaned Data\n",
    "\n",
    "For testing purposes in the pipeline, it makes sense for us to export the cleaned DataFrame to a CSV file.  This will allow us to use the cleaned data in the pipeline without having to run the cleaning steps again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.to_csv(\n",
    "    \"../tests/test_data/expected_transactions_clean_results.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 7 - Transfer the code from the Jupyter Notebook to a Python script, creating separate functions for each cleaning step\n",
    "\n",
    "### Epic 2 -Story 3 - Task 8 - Write tests for each cleaning function to ensure they work correctly\n",
    "\n",
    "### Epic 2 - Story 3 - Task 9 - Create a script to run the cleaning functions in sequence and log the process\n",
    "\n",
    "### Epic 2 - Story 3 - Task 10 - Add the transaction cleaning script to scripts/run and update any tests accordingly\n",
    "\n",
    "Jupyter Notebooks do not play nicely with CI/CD pipelines, so we will need to transfer the code from the Jupyter Notebook to a Python script.  We will create separate functions for each cleaning step and then write tests for each function to ensure they work correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
