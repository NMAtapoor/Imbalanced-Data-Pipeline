{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Different Dataset with different IR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file has been added to `data/raw` with the name `unclean_transactions.csv`.  This file contains an export of the transactions table from the database.  We'll use this for our exploratory data analysis and transformation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4174 entries, 0 to 4173\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Sex              4174 non-null   object \n",
      " 1    Length          4174 non-null   float64\n",
      " 2    Diameter        4174 non-null   float64\n",
      " 3    Height          4174 non-null   float64\n",
      " 4    Whole_weight    4174 non-null   float64\n",
      " 5    Shucked_weight  4174 non-null   float64\n",
      " 6    Viscera_weight  4174 non-null   float64\n",
      " 7    Shell_weight    4174 non-null   float64\n",
      " 8   Class            4174 non-null   string \n",
      "dtypes: float64(7), object(1), string(1)\n",
      "memory usage: 293.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.120</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.7775</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.330</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.7680</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>0.260</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.2165</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.165</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.8945</td>\n",
       "      <td>0.3145</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>0.320</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex   Length   Diameter   Height   Whole_weight   Shucked_weight  \\\n",
       "0   M    0.455      0.365    0.095         0.5140           0.2245   \n",
       "1   M    0.350      0.265    0.090         0.2255           0.0995   \n",
       "2   F    0.530      0.420    0.135         0.6770           0.2565   \n",
       "3   M    0.440      0.365    0.125         0.5160           0.2155   \n",
       "4   I    0.330      0.255    0.080         0.2050           0.0895   \n",
       "5   I    0.425      0.300    0.095         0.3515           0.1410   \n",
       "6   F    0.530      0.415    0.150         0.7775           0.2370   \n",
       "7   F    0.545      0.425    0.125         0.7680           0.2940   \n",
       "8   M    0.475      0.370    0.125         0.5095           0.2165   \n",
       "9   F    0.550      0.440    0.150         0.8945           0.3145   \n",
       "\n",
       "    Viscera_weight   Shell_weight     Class  \n",
       "0           0.1010          0.150  negative  \n",
       "1           0.0485          0.070  negative  \n",
       "2           0.1415          0.210  negative  \n",
       "3           0.1140          0.155  negative  \n",
       "4           0.0395          0.055  negative  \n",
       "5           0.0775          0.120  negative  \n",
       "6           0.1415          0.330  negative  \n",
       "7           0.1495          0.260  negative  \n",
       "8           0.1125          0.165  negative  \n",
       "9           0.1510          0.320  positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_cleaned_imb_dataset(path) -> pd.DataFrame:\n",
    "    imb_data = pd.read_csv(path)\n",
    "    imb_data[\"Class\"] = imb_data[\"Class\"].astype('string')\n",
    "    return imb_data\n",
    "\n",
    "\n",
    "\n",
    "clean_data_path = '../data/processed/cleaned_abalone.csv'\n",
    "cleaned_abalone_df = extract_cleaned_imb_dataset(clean_data_path)\n",
    "cleaned_abalone_df.info()\n",
    "cleaned_abalone_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count rows where Name == 'Alice'\n",
    "count_minority = (cleaned_abalone_df['Class'] == 'P').sum()\n",
    "count_majority = (cleaned_abalone_df['Class'] == 'N').sum()\n",
    "ratio = ((count_minority/count_majority))\n",
    "print(\"Ration of minority/majority:\", ratio)\n",
    "\n",
    "# Alternative using .shape[0]\n",
    "#count_alice_alt = df[df['Name'] == 'Alice'].shape[0]\n",
    "#print(\"Alternative count:\", count_alice_alt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "#import pandas as pd\n",
    "\n",
    "def resample_with_borderline_smote(X, y, target_minority_ratio, kind='borderline-1', random_state=42):\n",
    "    \n",
    "    # Ensure y is Series\n",
    "    y = pd.Series(y, name=y.name or \"target\")\n",
    "    # Apply BorderlineSMOTE\n",
    "    smote = BorderlineSMOTE(kind=kind, sampling_strategy=target_minority_ratio, random_state=random_state)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Merge into a single DataFrame\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    df_resampled[y.name] = y_resampled\n",
    "    return df_resampled\n",
    "\n",
    "\n",
    "\n",
    "# generate imbalance dataset versions\n",
    "\n",
    "def generate_imb_data_version(features, target, ratio_values: list[float])-> dict[str, pd.DataFrame]:\n",
    "    imb_datasets_dic = {}\n",
    "    for imb_ratio in ratio_values:\n",
    "        dataset_version = resample_with_borderline_smote(features, target, imb_ratio)\n",
    "        imb_datasets_dic[f\"abalone_df_{int(imb_ratio * 100)}\"] = dataset_version.copy()\n",
    "    return imb_datasets_dic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_versions_to_csv(data_dic: dict[str, pd.DataFrame]):\n",
    "    FILE_PATH = os.path.join(\n",
    "    os.path.dirname(__file__),\n",
    "    \"..\",\n",
    "    \"..\",\n",
    "    \"data\",\n",
    "    \"data_versions\"\n",
    ")\n",
    "    for name, df in data_dic.items():\n",
    "        df.to_csv(f\"{FILE_PATH}/{name}\", index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'M'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_13912\\2823271123.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m cleaned_abalone_df  = extract_cleaned_imb_dataset(clean_data_path)\n\u001b[32m      5\u001b[39m X = cleaned_abalone_df.drop(columns=[\u001b[33m\"Class\"\u001b[39m])  \u001b[38;5;66;03m# predictors\u001b[39;00m\n\u001b[32m      6\u001b[39m y = cleaned_abalone_df[\u001b[33m\"Class\"\u001b[39m]\n\u001b[32m      7\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m abalone_versions_dic = generate_imb_data_version(X, y, target_min_ratio)\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m abalone_versions_dic[\u001b[33m\"abalone_df_10\"\u001b[39m].info()\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_13912\\847659977.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(features, target, ratio_values)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m generate_imb_data_version(features, target, ratio_values: list[float])-> dict[str, pd.DataFrame]:\n\u001b[32m     22\u001b[39m     imb_datasets_dic = {}\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m imb_ratio \u001b[38;5;28;01min\u001b[39;00m ratio_values:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         dataset_version = resample_with_borderline_smote(features, target, imb_ratio)\n\u001b[32m     25\u001b[39m         imb_datasets_dic[f\"abalone_df_{int(imb_ratio * \u001b[32m100\u001b[39m)}\"] = dataset_version.copy()\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m imb_datasets_dic\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_13912\\847659977.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(X, y, target_minority_ratio, kind, random_state)\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Ensure y is Series\u001b[39;00m\n\u001b[32m      7\u001b[39m     y = pd.Series(y, name=y.name \u001b[38;5;28;01mor\u001b[39;00m \u001b[33m\"target\"\u001b[39m)\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Apply BorderlineSMOTE\u001b[39;00m\n\u001b[32m      9\u001b[39m     smote = BorderlineSMOTE(kind=kind, sampling_strategy=target_minority_ratio, random_state=random_state)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     X_resampled, y_resampled = smote.fit_resample(X, y)\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Merge into a single DataFrame\u001b[39;00m\n\u001b[32m     13\u001b[39m     df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
      "\u001b[32md:\\digitalfutures\\Atapoor_IDP\\Imbalanced-Data-Pipeline\\.venv\\Lib\\site-packages\\imblearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    198\u001b[39m \n\u001b[32m    199\u001b[39m         y_resampled : array-like of shape (n_samples_new,)\n\u001b[32m    200\u001b[39m             The corresponding label of `X_resampled`.\n\u001b[32m    201\u001b[39m         \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m super().fit_resample(X, y, **params)\n",
      "\u001b[32md:\\digitalfutures\\Atapoor_IDP\\Imbalanced-Data-Pipeline\\.venv\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1361\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m                 )\n\u001b[32m   1364\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32md:\\digitalfutures\\Atapoor_IDP\\Imbalanced-Data-Pipeline\\.venv\\Lib\\site-packages\\imblearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m     95\u001b[39m             The corresponding label of `X_resampled`.\n\u001b[32m     96\u001b[39m         \"\"\"\n\u001b[32m     97\u001b[39m         check_classification_targets(y)\n\u001b[32m     98\u001b[39m         arrays_transformer = ArraysTransformer(X, y)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m         X, y, binarize_y = self._check_X_y(X, y)\n\u001b[32m    100\u001b[39m \n\u001b[32m    101\u001b[39m         self.sampling_strategy_ = check_sampling_strategy(\n\u001b[32m    102\u001b[39m             self.sampling_strategy, y, self._sampling_type\n",
      "\u001b[32md:\\digitalfutures\\Atapoor_IDP\\Imbalanced-Data-Pipeline\\.venv\\Lib\\site-packages\\imblearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, accept_sparse)\u001b[39m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m _check_X_y(self, X, y, accept_sparse=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    154\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m accept_sparse \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    155\u001b[39m             accept_sparse = [\u001b[33m\"csr\"\u001b[39m, \u001b[33m\"csc\"\u001b[39m]\n\u001b[32m    156\u001b[39m         y, binarize_y = check_target_type(y, indicate_one_vs_all=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         X, y = validate_data(self, X=X, y=y, reset=\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse=accept_sparse)\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "\u001b[32md:\\digitalfutures\\Atapoor_IDP\\Imbalanced-Data-Pipeline\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2967\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"estimator\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m check_y_params:\n\u001b[32m   2968\u001b[39m                 check_y_params = {**default_check_params, **check_y_params}\n\u001b[32m   2969\u001b[39m             y = check_array(y, input_name=\u001b[33m\"y\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m             X, y = check_X_y(X, y, **check_params)\n\u001b[32m   2972\u001b[39m         out = X, y\n\u001b[32m   2973\u001b[39m \n\u001b[32m   2974\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m check_params.get(\u001b[33m\"ensure_2d\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[32md:\\digitalfutures\\Atapoor_IDP\\Imbalanced-Data-Pipeline\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m         )\n\u001b[32m   1365\u001b[39m \n\u001b[32m   1366\u001b[39m     ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m   1367\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     X = check_array(\n\u001b[32m   1369\u001b[39m         X,\n\u001b[32m   1370\u001b[39m         accept_sparse=accept_sparse,\n\u001b[32m   1371\u001b[39m         accept_large_sparse=accept_large_sparse,\n",
      "\u001b[32md:\\digitalfutures\\Atapoor_IDP\\Imbalanced-Data-Pipeline\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1050\u001b[39m                         )\n\u001b[32m   1051\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1052\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1053\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1055\u001b[39m                 raise ValueError(\n\u001b[32m   1056\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1057\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32md:\\digitalfutures\\Atapoor_IDP\\Imbalanced-Data-Pipeline\\.venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    754\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    755\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    756\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    758\u001b[39m \n\u001b[32m    759\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    760\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32md:\\digitalfutures\\Atapoor_IDP\\Imbalanced-Data-Pipeline\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2164\u001b[39m             )\n\u001b[32m   2165\u001b[39m         values = self._values\n\u001b[32m   2166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2167\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2168\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2171\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'M'"
     ]
    }
   ],
   "source": [
    "target_min_ratio = [0.05,0.10,0.15,0.20,0.25,0.30,0.35,0.40,0.45,\n",
    "                    0.50,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95,1.0]\n",
    "clean_data_path = '../data/processed/cleaned_abalone.csv'\n",
    "cleaned_abalone_df  = extract_cleaned_imb_dataset(clean_data_path)\n",
    "X = cleaned_abalone_df.drop(columns=[\"Class\"])  # predictors\n",
    "y = cleaned_abalone_df[\"Class\"] \n",
    "\n",
    "abalone_versions_dic = generate_imb_data_version(X, y, target_min_ratio)\n",
    "\n",
    "abalone_versions_dic[\"abalone_df_10\"].info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data_versions_to_csv(abalone_versions_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_abalone_csv(file_path) -> dict[str, pd.DataFrame]:\n",
    "    abalone_df_dic = { }\n",
    "    \n",
    "    for i in range(5, 101, 5):\n",
    "        full_path = f\"{file_path}abalone_df_{i}.csv\"\n",
    "        print(full_path)\n",
    "        abalone_df = pd.read_csv(full_path)\n",
    "        abalone_df[\"Class\"] = abalone_df[\"Class\"].astype('string')\n",
    "        abalone_df_dic[f\"abalone_df_{i}\"] = abalone_df.copy()\n",
    "        \n",
    "    return abalone_df_dic\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/data_versions/abalone_df_5.csv\n",
      "../data/data_versions/abalone_df_10.csv\n",
      "../data/data_versions/abalone_df_15.csv\n",
      "../data/data_versions/abalone_df_20.csv\n",
      "../data/data_versions/abalone_df_25.csv\n",
      "../data/data_versions/abalone_df_30.csv\n",
      "../data/data_versions/abalone_df_35.csv\n",
      "../data/data_versions/abalone_df_40.csv\n",
      "../data/data_versions/abalone_df_45.csv\n",
      "../data/data_versions/abalone_df_50.csv\n",
      "../data/data_versions/abalone_df_55.csv\n",
      "../data/data_versions/abalone_df_60.csv\n",
      "../data/data_versions/abalone_df_65.csv\n",
      "../data/data_versions/abalone_df_70.csv\n",
      "../data/data_versions/abalone_df_75.csv\n",
      "../data/data_versions/abalone_df_80.csv\n",
      "../data/data_versions/abalone_df_85.csv\n",
      "../data/data_versions/abalone_df_90.csv\n",
      "../data/data_versions/abalone_df_95.csv\n",
      "../data/data_versions/abalone_df_100.csv\n"
     ]
    }
   ],
   "source": [
    "my_path = \"../data/data_versions/\"\n",
    "my_abalone_dic = read_abalone_csv(my_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4556 entries, 0 to 4555\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0    Length          4556 non-null   float64\n",
      " 1    Diameter        4556 non-null   float64\n",
      " 2    Height          4556 non-null   float64\n",
      " 3    Whole_weight    4556 non-null   float64\n",
      " 4    Shucked_weight  4556 non-null   float64\n",
      " 5    Viscera_weight  4556 non-null   float64\n",
      " 6    Shell_weight    4556 non-null   float64\n",
      " 7   Sex_F            4556 non-null   float64\n",
      " 8   Sex_I            4556 non-null   float64\n",
      " 9   Sex_M            4556 non-null   float64\n",
      " 10  Class            4556 non-null   string \n",
      "dtypes: float64(10), string(1)\n",
      "memory usage: 391.7 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_I</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.575179</td>\n",
       "      <td>-0.432763</td>\n",
       "      <td>-1.064838</td>\n",
       "      <td>-0.642383</td>\n",
       "      <td>-0.608183</td>\n",
       "      <td>-0.726599</td>\n",
       "      <td>-0.638650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.450045</td>\n",
       "      <td>-1.441081</td>\n",
       "      <td>-1.184418</td>\n",
       "      <td>-1.231151</td>\n",
       "      <td>-1.171896</td>\n",
       "      <td>-1.205783</td>\n",
       "      <td>-1.213668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.049725</td>\n",
       "      <td>0.121812</td>\n",
       "      <td>-0.108206</td>\n",
       "      <td>-0.309734</td>\n",
       "      <td>-0.463873</td>\n",
       "      <td>-0.356942</td>\n",
       "      <td>-0.207386</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.700160</td>\n",
       "      <td>-0.432763</td>\n",
       "      <td>-0.347364</td>\n",
       "      <td>-0.638301</td>\n",
       "      <td>-0.648770</td>\n",
       "      <td>-0.607943</td>\n",
       "      <td>-0.602711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.616686</td>\n",
       "      <td>-1.541913</td>\n",
       "      <td>-1.423576</td>\n",
       "      <td>-1.272987</td>\n",
       "      <td>-1.216993</td>\n",
       "      <td>-1.287929</td>\n",
       "      <td>-1.321484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.825141</td>\n",
       "      <td>-1.088170</td>\n",
       "      <td>-1.064838</td>\n",
       "      <td>-0.974011</td>\n",
       "      <td>-0.984743</td>\n",
       "      <td>-0.941091</td>\n",
       "      <td>-0.854282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.049725</td>\n",
       "      <td>0.071396</td>\n",
       "      <td>0.250532</td>\n",
       "      <td>-0.104634</td>\n",
       "      <td>-0.551812</td>\n",
       "      <td>-0.356942</td>\n",
       "      <td>0.655142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.174706</td>\n",
       "      <td>0.172228</td>\n",
       "      <td>-0.347364</td>\n",
       "      <td>-0.124022</td>\n",
       "      <td>-0.294759</td>\n",
       "      <td>-0.283923</td>\n",
       "      <td>0.152001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.408538</td>\n",
       "      <td>-0.382347</td>\n",
       "      <td>-0.347364</td>\n",
       "      <td>-0.651566</td>\n",
       "      <td>-0.644261</td>\n",
       "      <td>-0.621634</td>\n",
       "      <td>-0.530834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.216366</td>\n",
       "      <td>0.323476</td>\n",
       "      <td>0.250532</td>\n",
       "      <td>0.134139</td>\n",
       "      <td>-0.202310</td>\n",
       "      <td>-0.270232</td>\n",
       "      <td>0.583264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Length   Diameter    Height   Whole_weight   Shucked_weight  \\\n",
       "0 -0.575179  -0.432763 -1.064838      -0.642383        -0.608183   \n",
       "1 -1.450045  -1.441081 -1.184418      -1.231151        -1.171896   \n",
       "2  0.049725   0.121812 -0.108206      -0.309734        -0.463873   \n",
       "3 -0.700160  -0.432763 -0.347364      -0.638301        -0.648770   \n",
       "4 -1.616686  -1.541913 -1.423576      -1.272987        -1.216993   \n",
       "5 -0.825141  -1.088170 -1.064838      -0.974011        -0.984743   \n",
       "6  0.049725   0.071396  0.250532      -0.104634        -0.551812   \n",
       "7  0.174706   0.172228 -0.347364      -0.124022        -0.294759   \n",
       "8 -0.408538  -0.382347 -0.347364      -0.651566        -0.644261   \n",
       "9  0.216366   0.323476  0.250532       0.134139        -0.202310   \n",
       "\n",
       "    Viscera_weight   Shell_weight  Sex_F  Sex_I  Sex_M Class  \n",
       "0        -0.726599      -0.638650    0.0    0.0    1.0     N  \n",
       "1        -1.205783      -1.213668    0.0    0.0    1.0     N  \n",
       "2        -0.356942      -0.207386    1.0    0.0    0.0     N  \n",
       "3        -0.607943      -0.602711    0.0    0.0    1.0     N  \n",
       "4        -1.287929      -1.321484    0.0    1.0    0.0     N  \n",
       "5        -0.941091      -0.854282    0.0    1.0    0.0     N  \n",
       "6        -0.356942       0.655142    1.0    0.0    0.0     N  \n",
       "7        -0.283923       0.152001    1.0    0.0    0.0     N  \n",
       "8        -0.621634      -0.530834    0.0    0.0    1.0     N  \n",
       "9        -0.270232       0.583264    1.0    0.0    0.0     P  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myab = my_abalone_dic[\"abalone_df_10\"]\n",
    "myab.info()\n",
    "myab.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.concat(abalone_versions_dic.values(), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.info()\n",
    "rows =joined_df.shape[0]\n",
    "print(f\"Total Rows: {rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My DB Port:5432 DB Name: pagila\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv(dotenv_path=\"../.env.dev\")\n",
    "# Read DB credentials from .env\n",
    "username = os.getenv(\"SOURCE_DB_USER\")\n",
    "password = os.getenv(\"SOURCE_DB_PASSWORD\")\n",
    "host = os.getenv(\"SOURCE_DB_HOST\")\n",
    "port = os.getenv(\"SOURCE_DB_PORT\")\n",
    "database = os.getenv(\"SOURCE_DB_NAME\")\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [25, 30, 35]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"My DB Port:{port} DB Name: {database}\")\n",
    "# Create SQLAlchemy engine\n",
    "db_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}\")\n",
    "\n",
    "# Write to PostgreSQL\n",
    "df.to_sql(\"atapoor2506\", db_engine, schema=\"de_2506_a\", if_exists=\"replace\", index=False)\n",
    "\n",
    "# Example dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_df_10.info()\n",
    "abalone_df_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"abalone_df_50\"\n",
    "all_df[name].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, \n",
    "    roc_auc_score, cohen_kappa_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "abalone_df = all_df[\"abalone_df_50\"]\n",
    "X = abalone_df.drop(columns=[\"Class\"])\n",
    "y = abalone_df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# SVM model\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_y_pred = svm_model.predict(X_test)\n",
    "svm_y_prob = svm_model.predict_proba(X_test)[:, 1]  # probability for positive class\n",
    "\n",
    "# Random forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,     # number of trees\n",
    "    max_depth=None,       # can tune if overfitting\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"   # helps with imbalanced data\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "rf_y_prob = rf_model.predict_proba(X_test)[:, 1] if len(y.unique()) == 2 else None\n",
    "\n",
    "# KNN \n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)  # Euclidean distance\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "knn_y_pred = knn_model.predict(X_test)\n",
    "knn_y_prob = knn_model.predict_proba(X_test)[:, 1] if len(y.unique()) == 2 else None\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
    "svm_f1 = f1_score(y_test, svm_y_pred, average='weighted')\n",
    "svm_precision = precision_score(y_test, svm_y_pred, average='weighted')\n",
    "svm_recall = recall_score(y_test, svm_y_pred, average='weighted')\n",
    "svm_kappa = cohen_kappa_score(y_test, svm_y_pred)\n",
    "svm_auc = roc_auc_score(y_test, svm_y_prob)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "rf_f1 = f1_score(y_test, rf_y_pred, average=\"weighted\")\n",
    "rf_precision = precision_score(y_test, rf_y_pred, average=\"weighted\")\n",
    "rf_recall = recall_score(y_test, rf_y_pred, average=\"weighted\")\n",
    "rf_kappa = cohen_kappa_score(y_test, rf_y_pred)\n",
    "rf_auc = roc_auc_score(y_test, rf_y_prob)\n",
    "\n",
    "knn_accuracy = accuracy_score(y_test, knn_y_pred)\n",
    "knn_f1 = f1_score(y_test, knn_y_pred, average=\"weighted\")\n",
    "knn_precision = precision_score(y_test, knn_y_pred, average=\"weighted\")\n",
    "knn_recall = recall_score(y_test, knn_y_pred, average=\"weighted\")\n",
    "knn_kappa = cohen_kappa_score(y_test, knn_y_pred)\n",
    "knn_auc = roc_auc_score(y_test, knn_y_prob)\n",
    "\n",
    "\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f} RF Accuracy: {rf_accuracy: .4f} KNN Accuracy: {knn_accuracy: .4f}\")\n",
    "print(f\"SVM F1-score: {svm_f1:.4f} RF F1-score: {rf_f1:.4f} KNN F1-score: {knn_f1:.4f}\")\n",
    "print(f\"SVM Precision: {svm_precision:.4f} RF Precision: {rf_precision:.4f} KNN Precision: {knn_precision:.4f}\")\n",
    "print(f\"SVM Recall: {svm_recall:.4f} RF Recall: {rf_recall:.4f} KNN Recall: {knn_recall:.4f}\")\n",
    "print(f\"SVM Cohen's Kappa: {svm_kappa:.4f} RF Cohen's Kappa: {rf_kappa:.4f} KNN Cohen's Kappa: {knn_kappa:.4f}\")\n",
    "print(f\"SVM AUC: {svm_auc:.4f} RF AUC: {rf_auc:.4f} KNN AUC: {knn_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "svm_accur = []\n",
    "svm_auc = []\n",
    "for key, value in all_df.items():\n",
    "    X = value.drop(columns=[\"Class\"])\n",
    "    y = value[\"Class\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    svm_y_pred = svm_model.predict(X_test)\n",
    "    svm_y_prob = svm_model.predict_proba(X_test)[:, 1]  # probability for positive class\n",
    "    datasets.append(key)\n",
    "    svm_accur.append(accuracy_score(y_test, svm_y_pred))\n",
    "    svm_auc.append(roc_auc_score(y_test, svm_y_prob))\n",
    "    \n",
    "    \n",
    "svm_metrics_df = pd.DataFrame({\"name\":datasets, \"Accuracy\": svm_accur, \"AUC\":svm_auc})\n",
    "print(svm_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Plot line chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(svm_metrics_df[\"name\"], svm_metrics_df[\"Accuracy\"], marker=\"o\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"name\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Line Chart Example\")\n",
    "\n",
    "# Show grid & plot\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Category\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "    \"Line1\": [2.3, 3.8, 1.5, 4.2, 3.0],\n",
    "    \"Line2\": [1.5, 2.5, 3.2, 2.8, 4.0],\n",
    "    \"Line3\": [3.0, 2.0, 4.5, 3.5, 2.2]\n",
    "})\n",
    "\n",
    "# Plot multiple lines\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for col in df.columns[1:]:   # skip 'Category' since it's x-axis\n",
    "    plt.plot(df[\"Category\"], df[col], marker=\"o\", label=col)\n",
    "\n",
    "# Add labels, title, legend\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Multiple Line Charts on Same Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many duplicates there are in the DataFrame\n",
    "duplicates = transactions.duplicated().sum()\n",
    "print(f'There are {duplicates} duplicate rows in the transactions DataFrame.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that there are some duplicate rows in the transactions DataFrame.  We will need to remove these duplicates before we can use the data for analysis or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the different date formats in the transaction_date column\n",
    "unique_date_formats = transactions['transaction_date'].unique()\n",
    "print(f'Different date formats in transaction_date: {unique_date_formats[:20]}')  # Show first 20 unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we will need to standardise the date format in the `transaction_date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the different data types in the amount column\n",
    "amount_types = transactions['amount'].map(type).unique()\n",
    "print(f'Different data types in amount column: {amount_types}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the amount is either a string or a float.  We will need to convert the amount column to a numeric type before we can use it for analysis or further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 2 - Handle Missing Values\n",
    "\n",
    "We are going to remove any incomplete rows from the DataFrame.  This will remove any rows that have missing values in any of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions.dropna(subset=[\"transaction_date\"])\n",
    "\n",
    "# remove rows with null values in amount from the transaction dataframe\n",
    "transactions = transactions.dropna(subset=[\"amount\"])\n",
    "\n",
    "# See information about the transactions dataframe\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have dropped 254 rows from the DataFrame that had missing values in any of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a check - this operation will be tested in the pipeline!\n",
    "transactions.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 3 - Standardise Date Format\n",
    "\n",
    "We saw that the `transaction_date` column has a mix of date formats.  We will standardise this to a single format.  You need to examine the date column and then identify and list ALL of the date formats that are present in the column.  You can then use this information to standardise the date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all dates into dd/mm/yyyy format - write function to handle the different types of date formats\n",
    "def standardise_date(date_str):\n",
    "    if pd.isna(date_str) or date_str == \"\":\n",
    "        return pd.NaT\n",
    "\n",
    "    formats = [\n",
    "        \"%Y/%m/%d\",\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%d %b %Y\",\n",
    "        \"%b %d, %Y\",\n",
    "        \"%d %B %Y\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%m/%d/%Y\",\n",
    "        \"%d/%m/%Y\",\n",
    "    ]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return pd.NaT\n",
    "\n",
    "\n",
    "# Apply the parse_date function to the transaction_date column\n",
    "transactions[\"transaction_date\"] = transactions[\"transaction_date\"].apply(\n",
    "    standardise_date\n",
    ")\n",
    "transactions[\"transaction_date\"] = transactions[\n",
    "    \"transaction_date\"\n",
    "].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "transactions = transactions.dropna(subset=[\"transaction_date\"])\n",
    "\n",
    "# Display the DataFrame info\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dates in the `transaction_date` column are now in the standardised format of `%d/%m/%Y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 4 - Convert Amount to Numeric\n",
    "\n",
    "Find out how many rows can be converted to numeric values in the `amount` column.  This will help us understand how many rows we can use for analysis or further processing.  We specifically want to convert the string to a float and understand how many rows will be converted and how many `NaN` values will be created as a result of this conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count convertible vs non-convertible amounts\n",
    "convertible = (\n",
    "    pd.to_numeric(transactions[\"amount\"], errors=\"coerce\").notna().sum()\n",
    ")\n",
    "print(f\"Convertible: {convertible}, NaNs: {len(transactions) - convertible}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values that generate the `NaN` values are those that cannot be converted to a float.  This includes any non-numeric characters or strings that do not represent a valid number.  We can check what these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original values that can't be converted to numeric\n",
    "unconvertible = transactions[\n",
    "    pd.to_numeric(transactions[\"amount\"], errors=\"coerce\").isna()\n",
    "][\"amount\"].unique()\n",
    "print(unconvertible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms to use that a number of rows in the `amount` column cannot be converted to a float as they contain the string `INVALID`.  We can safely remove these rows from the DataFrame as they will not be useful for analysis or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert and drop NaNs in one operation\n",
    "transactions[\"amount\"] = pd.to_numeric(transactions[\"amount\"], errors=\"coerce\")\n",
    "transactions.dropna(subset=[\"amount\"], inplace=True)\n",
    "\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have cleaned the `amount` column by converting it to a numeric type and dropping any rows that contain `NaN` values in this column.  This will allow us to use the `amount` column for analysis or further processing.\n",
    "\n",
    "This concluldes the cleaning of the `transactions` DataFrame.  We have removed any rows with missing values, standardised the date format, removed duplicates, and converted the `amount` column to a numeric type.  The DataFrame is now ready for analysis or further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 5 - Remove Duplicates\n",
    "\n",
    "Check to see how many duplicates there are now once the data has been cleaned a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many duplicates there are in the DataFrame\n",
    "duplicates = transactions.duplicated().sum()\n",
    "print(f\"There are {duplicates} duplicate rows in the transactions DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates\n",
    "\n",
    "transactions.drop_duplicates(inplace=True)\n",
    "\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should have removed the 503 duplicated rows from the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the indexes\n",
    "\n",
    "> This was added after the COMPONENT tests for customers data failed due to index conflicts - as we modified the DataFrame, the indexes were no longer sequential.\n",
    "\n",
    "We can see that the indexes are now out of order, so we will reset them to be sequential again.\n",
    "\n",
    "The COMPONENT tests for the transactions data set will also need to be updated/added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 6 - Save the Cleaned Data\n",
    "\n",
    "For testing purposes in the pipeline, it makes sense for us to export the cleaned DataFrame to a CSV file.  This will allow us to use the cleaned data in the pipeline without having to run the cleaning steps again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.to_csv(\n",
    "    \"../tests/test_data/expected_transactions_clean_results.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 7 - Transfer the code from the Jupyter Notebook to a Python script, creating separate functions for each cleaning step\n",
    "\n",
    "### Epic 2 -Story 3 - Task 8 - Write tests for each cleaning function to ensure they work correctly\n",
    "\n",
    "### Epic 2 - Story 3 - Task 9 - Create a script to run the cleaning functions in sequence and log the process\n",
    "\n",
    "### Epic 2 - Story 3 - Task 10 - Add the transaction cleaning script to scripts/run and update any tests accordingly\n",
    "\n",
    "Jupyter Notebooks do not play nicely with CI/CD pipelines, so we will need to transfer the code from the Jupyter Notebook to a Python script.  We will create separate functions for each cleaning step and then write tests for each function to ensure they work correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
