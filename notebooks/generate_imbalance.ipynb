{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Different Dataset with different IR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file has been added to `data/raw` with the name `unclean_transactions.csv`.  This file contains an export of the transactions table from the database.  We'll use this for our exploratory data analysis and transformation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cleaned_imb_dataset(path) -> pd.DataFrame:\n",
    "    imb_data = pd.read_csv(path)\n",
    "    imb_data[\"Class\"] = imb_data[\"Class\"].astype('string')\n",
    "    return imb_data\n",
    "\n",
    "\n",
    "\n",
    "clean_data_path = '../data/processed/cleaned_abalone.csv'\n",
    "cleaned_abalone_df = extract_cleaned_imb_dataset(clean_data_path)\n",
    "cleaned_abalone_df.info()\n",
    "cleaned_abalone_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count rows where Name == 'Alice'\n",
    "count_minority = (cleaned_abalone_df['Class'] == 'P').sum()\n",
    "count_majority = (cleaned_abalone_df['Class'] == 'N').sum()\n",
    "ratio = ((count_minority/count_majority))\n",
    "print(\"Ration of minority/majority:\", ratio)\n",
    "\n",
    "# Alternative using .shape[0]\n",
    "#count_alice_alt = df[df['Name'] == 'Alice'].shape[0]\n",
    "#print(\"Alternative count:\", count_alice_alt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "#import pandas as pd\n",
    "\n",
    "def resample_with_borderline_smote(X, y, target_minority_ratio, kind='borderline-1', random_state=42):\n",
    "    \n",
    "    # Ensure y is Series\n",
    "    y = pd.Series(y, name=y.name or \"target\")\n",
    "    # Apply BorderlineSMOTE\n",
    "    smote = BorderlineSMOTE(kind=kind, sampling_strategy=target_minority_ratio, random_state=random_state)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Merge into a single DataFrame\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    df_resampled[y.name] = y_resampled\n",
    "    return df_resampled\n",
    "\n",
    "\n",
    "\n",
    "# generate imbalance dataset versions\n",
    "\n",
    "def generate_imb_data_version(features, target, ratio_values: list[float])-> dict[str, pd.DataFrame]:\n",
    "    imb_datasets_dic = {}\n",
    "    for imb_ratio in ratio_values:\n",
    "        dataset_version = resample_with_borderline_smote(features, target, imb_ratio)\n",
    "        imb_datasets_dic[f\"abalone_df_{int(imb_ratio * 100)}\"] = dataset_version.copy()\n",
    "    return imb_datasets_dic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_versions_to_csv(data_dic: dict[str, pd.DataFrame]):\n",
    "    for key, value in data_dic.items():\n",
    "        value.to_csv(f\"../data/data_versions/{key}.csv\", index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_min_ratio = [0.05,0.10,0.15,0.20,0.25,0.30,0.35,0.40,0.45,\n",
    "                    0.50,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95,1.0]\n",
    "clean_data_path = '../data/processed/cleaned_abalone.csv'\n",
    "cleaned_abalone_df  = extract_cleaned_imb_dataset(clean_data_path)\n",
    "X = cleaned_abalone_df.drop(columns=[\"Class\"])  # predictors\n",
    "y = cleaned_abalone_df[\"Class\"] \n",
    "\n",
    "abalone_versions_dic = generate_imb_data_version(X, y, target_min_ratio)\n",
    "\n",
    "abalone_versions_dic[\"abalone_df_10\"].info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data_versions_to_csv(abalone_versions_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_abalone_csv(file_path) -> dict[str, pd.DataFrame]:\n",
    "    abalone_df_dic = { }\n",
    "    \n",
    "    for i in range(5, 101, 5):\n",
    "        full_path = f\"{file_path}abalone_df_{i}.csv\"\n",
    "        print(full_path)\n",
    "        abalone_df = pd.read_csv(full_path)\n",
    "        abalone_df[\"Class\"] = abalone_df[\"Class\"].astype('string')\n",
    "        abalone_df_dic[f\"abalone_df_{i}\"] = abalone_df.copy()\n",
    "        \n",
    "    return abalone_df_dic\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/data_versions/abalone_df_5.csv\n",
      "../data/data_versions/abalone_df_10.csv\n",
      "../data/data_versions/abalone_df_15.csv\n",
      "../data/data_versions/abalone_df_20.csv\n",
      "../data/data_versions/abalone_df_25.csv\n",
      "../data/data_versions/abalone_df_30.csv\n",
      "../data/data_versions/abalone_df_35.csv\n",
      "../data/data_versions/abalone_df_40.csv\n",
      "../data/data_versions/abalone_df_45.csv\n",
      "../data/data_versions/abalone_df_50.csv\n",
      "../data/data_versions/abalone_df_55.csv\n",
      "../data/data_versions/abalone_df_60.csv\n",
      "../data/data_versions/abalone_df_65.csv\n",
      "../data/data_versions/abalone_df_70.csv\n",
      "../data/data_versions/abalone_df_75.csv\n",
      "../data/data_versions/abalone_df_80.csv\n",
      "../data/data_versions/abalone_df_85.csv\n",
      "../data/data_versions/abalone_df_90.csv\n",
      "../data/data_versions/abalone_df_95.csv\n",
      "../data/data_versions/abalone_df_100.csv\n"
     ]
    }
   ],
   "source": [
    "my_path = \"../data/data_versions/\"\n",
    "my_abalone_dic = read_abalone_csv(my_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4556 entries, 0 to 4555\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0    Length          4556 non-null   float64\n",
      " 1    Diameter        4556 non-null   float64\n",
      " 2    Height          4556 non-null   float64\n",
      " 3    Whole_weight    4556 non-null   float64\n",
      " 4    Shucked_weight  4556 non-null   float64\n",
      " 5    Viscera_weight  4556 non-null   float64\n",
      " 6    Shell_weight    4556 non-null   float64\n",
      " 7   Sex_F            4556 non-null   float64\n",
      " 8   Sex_I            4556 non-null   float64\n",
      " 9   Sex_M            4556 non-null   float64\n",
      " 10  Class            4556 non-null   string \n",
      "dtypes: float64(10), string(1)\n",
      "memory usage: 391.7 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole_weight</th>\n",
       "      <th>Shucked_weight</th>\n",
       "      <th>Viscera_weight</th>\n",
       "      <th>Shell_weight</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_I</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.575179</td>\n",
       "      <td>-0.432763</td>\n",
       "      <td>-1.064838</td>\n",
       "      <td>-0.642383</td>\n",
       "      <td>-0.608183</td>\n",
       "      <td>-0.726599</td>\n",
       "      <td>-0.638650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.450045</td>\n",
       "      <td>-1.441081</td>\n",
       "      <td>-1.184418</td>\n",
       "      <td>-1.231151</td>\n",
       "      <td>-1.171896</td>\n",
       "      <td>-1.205783</td>\n",
       "      <td>-1.213668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.049725</td>\n",
       "      <td>0.121812</td>\n",
       "      <td>-0.108206</td>\n",
       "      <td>-0.309734</td>\n",
       "      <td>-0.463873</td>\n",
       "      <td>-0.356942</td>\n",
       "      <td>-0.207386</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.700160</td>\n",
       "      <td>-0.432763</td>\n",
       "      <td>-0.347364</td>\n",
       "      <td>-0.638301</td>\n",
       "      <td>-0.648770</td>\n",
       "      <td>-0.607943</td>\n",
       "      <td>-0.602711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.616686</td>\n",
       "      <td>-1.541913</td>\n",
       "      <td>-1.423576</td>\n",
       "      <td>-1.272987</td>\n",
       "      <td>-1.216993</td>\n",
       "      <td>-1.287929</td>\n",
       "      <td>-1.321484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.825141</td>\n",
       "      <td>-1.088170</td>\n",
       "      <td>-1.064838</td>\n",
       "      <td>-0.974011</td>\n",
       "      <td>-0.984743</td>\n",
       "      <td>-0.941091</td>\n",
       "      <td>-0.854282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.049725</td>\n",
       "      <td>0.071396</td>\n",
       "      <td>0.250532</td>\n",
       "      <td>-0.104634</td>\n",
       "      <td>-0.551812</td>\n",
       "      <td>-0.356942</td>\n",
       "      <td>0.655142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.174706</td>\n",
       "      <td>0.172228</td>\n",
       "      <td>-0.347364</td>\n",
       "      <td>-0.124022</td>\n",
       "      <td>-0.294759</td>\n",
       "      <td>-0.283923</td>\n",
       "      <td>0.152001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.408538</td>\n",
       "      <td>-0.382347</td>\n",
       "      <td>-0.347364</td>\n",
       "      <td>-0.651566</td>\n",
       "      <td>-0.644261</td>\n",
       "      <td>-0.621634</td>\n",
       "      <td>-0.530834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.216366</td>\n",
       "      <td>0.323476</td>\n",
       "      <td>0.250532</td>\n",
       "      <td>0.134139</td>\n",
       "      <td>-0.202310</td>\n",
       "      <td>-0.270232</td>\n",
       "      <td>0.583264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Length   Diameter    Height   Whole_weight   Shucked_weight  \\\n",
       "0 -0.575179  -0.432763 -1.064838      -0.642383        -0.608183   \n",
       "1 -1.450045  -1.441081 -1.184418      -1.231151        -1.171896   \n",
       "2  0.049725   0.121812 -0.108206      -0.309734        -0.463873   \n",
       "3 -0.700160  -0.432763 -0.347364      -0.638301        -0.648770   \n",
       "4 -1.616686  -1.541913 -1.423576      -1.272987        -1.216993   \n",
       "5 -0.825141  -1.088170 -1.064838      -0.974011        -0.984743   \n",
       "6  0.049725   0.071396  0.250532      -0.104634        -0.551812   \n",
       "7  0.174706   0.172228 -0.347364      -0.124022        -0.294759   \n",
       "8 -0.408538  -0.382347 -0.347364      -0.651566        -0.644261   \n",
       "9  0.216366   0.323476  0.250532       0.134139        -0.202310   \n",
       "\n",
       "    Viscera_weight   Shell_weight  Sex_F  Sex_I  Sex_M Class  \n",
       "0        -0.726599      -0.638650    0.0    0.0    1.0     N  \n",
       "1        -1.205783      -1.213668    0.0    0.0    1.0     N  \n",
       "2        -0.356942      -0.207386    1.0    0.0    0.0     N  \n",
       "3        -0.607943      -0.602711    0.0    0.0    1.0     N  \n",
       "4        -1.287929      -1.321484    0.0    1.0    0.0     N  \n",
       "5        -0.941091      -0.854282    0.0    1.0    0.0     N  \n",
       "6        -0.356942       0.655142    1.0    0.0    0.0     N  \n",
       "7        -0.283923       0.152001    1.0    0.0    0.0     N  \n",
       "8        -0.621634      -0.530834    0.0    0.0    1.0     N  \n",
       "9        -0.270232       0.583264    1.0    0.0    0.0     P  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myab = my_abalone_dic[\"abalone_df_10\"]\n",
    "myab.info()\n",
    "myab.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.concat(abalone_versions_dic.values(), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.info()\n",
    "rows =joined_df.shape[0]\n",
    "print(f\"Total Rows: {rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv(dotenv_path=\"../.env.dev\")\n",
    "# Read DB credentials from .env\n",
    "username = os.getenv(\"SOURCE_DB_USER\")\n",
    "password = os.getenv(\"SOURCE_DB_PASSWORD\")\n",
    "host = os.getenv(\"SOURCE_DB_HOST\")\n",
    "port = os.getenv(\"SOURCE_DB_PORT\")\n",
    "database = os.getenv(\"SOURCE_DB_NAME\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"My DB Port:{port} DB Name: {database}\")\n",
    "# Create SQLAlchemy engine\n",
    "db_engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}\")\n",
    "\n",
    "# Write to PostgreSQL\n",
    "joined_df.to_sql(\"abaloneTable\", db_engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(\"âœ… Data written to PostgreSQL successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_df_10.info()\n",
    "abalone_df_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"abalone_df_50\"\n",
    "all_df[name].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, \n",
    "    roc_auc_score, cohen_kappa_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "abalone_df = all_df[\"abalone_df_50\"]\n",
    "X = abalone_df.drop(columns=[\"Class\"])\n",
    "y = abalone_df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# SVM model\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_y_pred = svm_model.predict(X_test)\n",
    "svm_y_prob = svm_model.predict_proba(X_test)[:, 1]  # probability for positive class\n",
    "\n",
    "# Random forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,     # number of trees\n",
    "    max_depth=None,       # can tune if overfitting\n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"   # helps with imbalanced data\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "rf_y_prob = rf_model.predict_proba(X_test)[:, 1] if len(y.unique()) == 2 else None\n",
    "\n",
    "# KNN \n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)  # Euclidean distance\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "knn_y_pred = knn_model.predict(X_test)\n",
    "knn_y_prob = knn_model.predict_proba(X_test)[:, 1] if len(y.unique()) == 2 else None\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
    "svm_f1 = f1_score(y_test, svm_y_pred, average='weighted')\n",
    "svm_precision = precision_score(y_test, svm_y_pred, average='weighted')\n",
    "svm_recall = recall_score(y_test, svm_y_pred, average='weighted')\n",
    "svm_kappa = cohen_kappa_score(y_test, svm_y_pred)\n",
    "svm_auc = roc_auc_score(y_test, svm_y_prob)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "rf_f1 = f1_score(y_test, rf_y_pred, average=\"weighted\")\n",
    "rf_precision = precision_score(y_test, rf_y_pred, average=\"weighted\")\n",
    "rf_recall = recall_score(y_test, rf_y_pred, average=\"weighted\")\n",
    "rf_kappa = cohen_kappa_score(y_test, rf_y_pred)\n",
    "rf_auc = roc_auc_score(y_test, rf_y_prob)\n",
    "\n",
    "knn_accuracy = accuracy_score(y_test, knn_y_pred)\n",
    "knn_f1 = f1_score(y_test, knn_y_pred, average=\"weighted\")\n",
    "knn_precision = precision_score(y_test, knn_y_pred, average=\"weighted\")\n",
    "knn_recall = recall_score(y_test, knn_y_pred, average=\"weighted\")\n",
    "knn_kappa = cohen_kappa_score(y_test, knn_y_pred)\n",
    "knn_auc = roc_auc_score(y_test, knn_y_prob)\n",
    "\n",
    "\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f} RF Accuracy: {rf_accuracy: .4f} KNN Accuracy: {knn_accuracy: .4f}\")\n",
    "print(f\"SVM F1-score: {svm_f1:.4f} RF F1-score: {rf_f1:.4f} KNN F1-score: {knn_f1:.4f}\")\n",
    "print(f\"SVM Precision: {svm_precision:.4f} RF Precision: {rf_precision:.4f} KNN Precision: {knn_precision:.4f}\")\n",
    "print(f\"SVM Recall: {svm_recall:.4f} RF Recall: {rf_recall:.4f} KNN Recall: {knn_recall:.4f}\")\n",
    "print(f\"SVM Cohen's Kappa: {svm_kappa:.4f} RF Cohen's Kappa: {rf_kappa:.4f} KNN Cohen's Kappa: {knn_kappa:.4f}\")\n",
    "print(f\"SVM AUC: {svm_auc:.4f} RF AUC: {rf_auc:.4f} KNN AUC: {knn_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "svm_accur = []\n",
    "svm_auc = []\n",
    "for key, value in all_df.items():\n",
    "    X = value.drop(columns=[\"Class\"])\n",
    "    y = value[\"Class\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    svm_y_pred = svm_model.predict(X_test)\n",
    "    svm_y_prob = svm_model.predict_proba(X_test)[:, 1]  # probability for positive class\n",
    "    datasets.append(key)\n",
    "    svm_accur.append(accuracy_score(y_test, svm_y_pred))\n",
    "    svm_auc.append(roc_auc_score(y_test, svm_y_prob))\n",
    "    \n",
    "    \n",
    "svm_metrics_df = pd.DataFrame({\"name\":datasets, \"Accuracy\": svm_accur, \"AUC\":svm_auc})\n",
    "print(svm_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Plot line chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(svm_metrics_df[\"name\"], svm_metrics_df[\"Accuracy\"], marker=\"o\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"name\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Line Chart Example\")\n",
    "\n",
    "# Show grid & plot\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Category\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "    \"Line1\": [2.3, 3.8, 1.5, 4.2, 3.0],\n",
    "    \"Line2\": [1.5, 2.5, 3.2, 2.8, 4.0],\n",
    "    \"Line3\": [3.0, 2.0, 4.5, 3.5, 2.2]\n",
    "})\n",
    "\n",
    "# Plot multiple lines\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for col in df.columns[1:]:   # skip 'Category' since it's x-axis\n",
    "    plt.plot(df[\"Category\"], df[col], marker=\"o\", label=col)\n",
    "\n",
    "# Add labels, title, legend\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Multiple Line Charts on Same Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many duplicates there are in the DataFrame\n",
    "duplicates = transactions.duplicated().sum()\n",
    "print(f'There are {duplicates} duplicate rows in the transactions DataFrame.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that there are some duplicate rows in the transactions DataFrame.  We will need to remove these duplicates before we can use the data for analysis or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the different date formats in the transaction_date column\n",
    "unique_date_formats = transactions['transaction_date'].unique()\n",
    "print(f'Different date formats in transaction_date: {unique_date_formats[:20]}')  # Show first 20 unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we will need to standardise the date format in the `transaction_date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the different data types in the amount column\n",
    "amount_types = transactions['amount'].map(type).unique()\n",
    "print(f'Different data types in amount column: {amount_types}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the amount is either a string or a float.  We will need to convert the amount column to a numeric type before we can use it for analysis or further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 2 - Handle Missing Values\n",
    "\n",
    "We are going to remove any incomplete rows from the DataFrame.  This will remove any rows that have missing values in any of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = transactions.dropna(subset=[\"transaction_date\"])\n",
    "\n",
    "# remove rows with null values in amount from the transaction dataframe\n",
    "transactions = transactions.dropna(subset=[\"amount\"])\n",
    "\n",
    "# See information about the transactions dataframe\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have dropped 254 rows from the DataFrame that had missing values in any of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a check - this operation will be tested in the pipeline!\n",
    "transactions.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 3 - Standardise Date Format\n",
    "\n",
    "We saw that the `transaction_date` column has a mix of date formats.  We will standardise this to a single format.  You need to examine the date column and then identify and list ALL of the date formats that are present in the column.  You can then use this information to standardise the date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all dates into dd/mm/yyyy format - write function to handle the different types of date formats\n",
    "def standardise_date(date_str):\n",
    "    if pd.isna(date_str) or date_str == \"\":\n",
    "        return pd.NaT\n",
    "\n",
    "    formats = [\n",
    "        \"%Y/%m/%d\",\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%d %b %Y\",\n",
    "        \"%b %d, %Y\",\n",
    "        \"%d %B %Y\",\n",
    "        \"%d-%m-%Y\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%m/%d/%Y\",\n",
    "        \"%d/%m/%Y\",\n",
    "    ]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return pd.NaT\n",
    "\n",
    "\n",
    "# Apply the parse_date function to the transaction_date column\n",
    "transactions[\"transaction_date\"] = transactions[\"transaction_date\"].apply(\n",
    "    standardise_date\n",
    ")\n",
    "transactions[\"transaction_date\"] = transactions[\n",
    "    \"transaction_date\"\n",
    "].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "transactions = transactions.dropna(subset=[\"transaction_date\"])\n",
    "\n",
    "# Display the DataFrame info\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dates in the `transaction_date` column are now in the standardised format of `%d/%m/%Y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 4 - Convert Amount to Numeric\n",
    "\n",
    "Find out how many rows can be converted to numeric values in the `amount` column.  This will help us understand how many rows we can use for analysis or further processing.  We specifically want to convert the string to a float and understand how many rows will be converted and how many `NaN` values will be created as a result of this conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count convertible vs non-convertible amounts\n",
    "convertible = (\n",
    "    pd.to_numeric(transactions[\"amount\"], errors=\"coerce\").notna().sum()\n",
    ")\n",
    "print(f\"Convertible: {convertible}, NaNs: {len(transactions) - convertible}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values that generate the `NaN` values are those that cannot be converted to a float.  This includes any non-numeric characters or strings that do not represent a valid number.  We can check what these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original values that can't be converted to numeric\n",
    "unconvertible = transactions[\n",
    "    pd.to_numeric(transactions[\"amount\"], errors=\"coerce\").isna()\n",
    "][\"amount\"].unique()\n",
    "print(unconvertible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms to use that a number of rows in the `amount` column cannot be converted to a float as they contain the string `INVALID`.  We can safely remove these rows from the DataFrame as they will not be useful for analysis or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert and drop NaNs in one operation\n",
    "transactions[\"amount\"] = pd.to_numeric(transactions[\"amount\"], errors=\"coerce\")\n",
    "transactions.dropna(subset=[\"amount\"], inplace=True)\n",
    "\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have cleaned the `amount` column by converting it to a numeric type and dropping any rows that contain `NaN` values in this column.  This will allow us to use the `amount` column for analysis or further processing.\n",
    "\n",
    "This concluldes the cleaning of the `transactions` DataFrame.  We have removed any rows with missing values, standardised the date format, removed duplicates, and converted the `amount` column to a numeric type.  The DataFrame is now ready for analysis or further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 5 - Remove Duplicates\n",
    "\n",
    "Check to see how many duplicates there are now once the data has been cleaned a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many duplicates there are in the DataFrame\n",
    "duplicates = transactions.duplicated().sum()\n",
    "print(f\"There are {duplicates} duplicate rows in the transactions DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates\n",
    "\n",
    "transactions.drop_duplicates(inplace=True)\n",
    "\n",
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should have removed the 503 duplicated rows from the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the indexes\n",
    "\n",
    "> This was added after the COMPONENT tests for customers data failed due to index conflicts - as we modified the DataFrame, the indexes were no longer sequential.\n",
    "\n",
    "We can see that the indexes are now out of order, so we will reset them to be sequential again.\n",
    "\n",
    "The COMPONENT tests for the transactions data set will also need to be updated/added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 6 - Save the Cleaned Data\n",
    "\n",
    "For testing purposes in the pipeline, it makes sense for us to export the cleaned DataFrame to a CSV file.  This will allow us to use the cleaned data in the pipeline without having to run the cleaning steps again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.to_csv(\n",
    "    \"../tests/test_data/expected_transactions_clean_results.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epic 2 - Story 3 - Task 7 - Transfer the code from the Jupyter Notebook to a Python script, creating separate functions for each cleaning step\n",
    "\n",
    "### Epic 2 -Story 3 - Task 8 - Write tests for each cleaning function to ensure they work correctly\n",
    "\n",
    "### Epic 2 - Story 3 - Task 9 - Create a script to run the cleaning functions in sequence and log the process\n",
    "\n",
    "### Epic 2 - Story 3 - Task 10 - Add the transaction cleaning script to scripts/run and update any tests accordingly\n",
    "\n",
    "Jupyter Notebooks do not play nicely with CI/CD pipelines, so we will need to transfer the code from the Jupyter Notebook to a Python script.  We will create separate functions for each cleaning step and then write tests for each function to ensure they work correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
